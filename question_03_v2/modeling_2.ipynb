{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries needed for the project - if you don't have any of them, install them using pip install <library name>\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt # for plotting the images\n",
    "\n",
    "\n",
    "# libraries for the supervised deep learning classification model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the image size to resize all images to\n",
    "img_size = (64, 64)\n",
    "\n",
    "# Get the list of image file names\n",
    "filenames = os.listdir(\"../question_01/dataset\")\n",
    "\n",
    "# Initialize lists to store the images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load the images and create labels\n",
    "for filename in filenames:\n",
    "    # Load the image and resize it\n",
    "    img = load_img(os.path.join(\"../question_01/dataset\", filename), target_size=img_size)\n",
    "    # Convert the image to a numpy array and normalize the pixel values\n",
    "    images.append(img_to_array(img) / 255.0)\n",
    "    # Create a label based on the filename\n",
    "    if \"real\" in filename:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 112\n",
      "Number of test examples: 38\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(X_train))\n",
    "print(\"Number of test examples:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    # Initialize the model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a convolutional layer with 32 filters, a 3x3 kernel, and relu activation function\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "\n",
    "    # Add a max pooling layer with a 2x2 pool size\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Add another convolutional layer with 64 filters, a 3x3 kernel, and relu activation function\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    # Add another max pooling layer with a 2x2 pool size\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Flatten the tensor output from the previous layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Add a dense layer\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Print a summary of the model\n",
    "    model.summary()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_30 (Conv2D)          (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPoolin  (None, 31, 31, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPoolin  (None, 14, 14, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 12544)             0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 128)               1605760   \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,625,281\n",
      "Trainable params: 1,625,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 54ms/step - loss: 0.6938 - accuracy: 0.5089 - val_loss: 0.7018 - val_accuracy: 0.6053\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.6943 - accuracy: 0.5357 - val_loss: 1.0402 - val_accuracy: 0.3947\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.6915 - accuracy: 0.5357 - val_loss: 1.0150 - val_accuracy: 0.3947\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.6913 - accuracy: 0.5357 - val_loss: 1.0145 - val_accuracy: 0.3947\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 0.6918 - accuracy: 0.5357 - val_loss: 1.0536 - val_accuracy: 0.3947\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 0.6916 - accuracy: 0.5357 - val_loss: 1.0892 - val_accuracy: 0.3947\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.6915 - accuracy: 0.5357 - val_loss: 1.0272 - val_accuracy: 0.3947\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.6919 - accuracy: 0.5357 - val_loss: 1.0765 - val_accuracy: 0.3947\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 0.6907 - accuracy: 0.5357 - val_loss: 1.0675 - val_accuracy: 0.3947\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 0.6910 - accuracy: 0.5357 - val_loss: 1.0257 - val_accuracy: 0.3947\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "THIS HAS BEEN REMOVED IN V2 - DATA AUGMENTATION PROVIDED SOME SIGNIFICANT ISSUES - NEED TO REVISIT WHEN TIME PERMITS \n",
    "HOWEVER I WOULD OF PERFORMED DATA AUGMENTATION IN THE FOLLOWING WAY TO PREVENT OVERFITTING OF THE MODEL. I TRIED IT \n",
    "USING TWO DIFFERENT METHODS BUT IT DID NOT WORK AS EXPECTED. I EXPECTED THE FIRST 'DATAGEN' WAS ACTUALLY CAUSING OVERFITTING \n",
    "SO I TRIED TO USE LESS PARAMETERS IN THE SECOND 'DATAGEN' BUT IT DID NOT WORK EITHER. \n",
    "\n",
    "THE NEXT THING I WOULD OF TRIED IS ALTERING THE MODEL TO PREVENT OVERFITTING. I WOULD OF TRIED TO ADD DROPOUT LAYERS AS WELL \n",
    "\n",
    "'''\n",
    "# data augmentation parameters - used to create more training data and prevent overfitting of the model\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rescale=1./255,\n",
    "#     rotation_range=20,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     fill_mode='nearest'\n",
    "# )\n",
    "\n",
    "# datagen = ImageDataGenerator(\n",
    "#     rescale=1. / 255,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# # fit the data augmentation generator to the training data\n",
    "# datagen.fit(X_train)\n",
    "\n",
    "# # fit the model on the augmented training data\n",
    "# history = model.fit(datagen.flow(X_train, y_train, batch_size=16), epochs=10, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 86ms/step - loss: 0.9194 - accuracy: 0.4911 - val_loss: 0.7018 - val_accuracy: 0.6053\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.7527 - accuracy: 0.4554 - val_loss: 0.7119 - val_accuracy: 0.3947\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.6857 - accuracy: 0.5357 - val_loss: 0.7081 - val_accuracy: 0.3947\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.6812 - accuracy: 0.5357 - val_loss: 0.7010 - val_accuracy: 0.4211\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.6709 - accuracy: 0.6429 - val_loss: 0.7006 - val_accuracy: 0.4737\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.6543 - accuracy: 0.7857 - val_loss: 0.6963 - val_accuracy: 0.4737\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.6303 - accuracy: 0.8036 - val_loss: 0.7071 - val_accuracy: 0.4474\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.6009 - accuracy: 0.7500 - val_loss: 0.7003 - val_accuracy: 0.5263\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.5584 - accuracy: 0.8125 - val_loss: 0.6999 - val_accuracy: 0.5526\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.5026 - accuracy: 0.8304 - val_loss: 0.7082 - val_accuracy: 0.5263\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7082 - accuracy: 0.5263\n",
      "Test accuracy: 0.5263158082962036\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy of the model on the test data\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path to the testing data directory\n",
    "test_data_dir = \"..\\\\rd_test_dataset\\\\rd_test_dataset\"\n",
    "\n",
    "# Set the image size\n",
    "img_size = (64, 64)\n",
    "\n",
    "# Initialize lists to store the preprocessed images, predictions, and labels\n",
    "test_images = []\n",
    "predictions = []\n",
    "\n",
    "# Load and preprocess the testing images\n",
    "for filename in os.listdir(test_data_dir):\n",
    "    # Exclude the \".DS_Store\" file because it was causing problems - not on MAC so not needed\n",
    "    if not filename.startswith(\".\"):\n",
    "        # Check if the file is a PNG or JPEG image\n",
    "        if filename.endswith(\".png\") or filename.endswith(\".jpg\") or filename.endswith(\".jpeg\"):\n",
    "            # Load the image and resize it\n",
    "            img = load_img(os.path.join(test_data_dir, filename), target_size=img_size)\n",
    "            # convert the image to a np array and normalize the values\n",
    "            img = img_to_array(img) / 255.0\n",
    "            # append the preprocessed image to the list\n",
    "            test_images.append(img)\n",
    "\n",
    "# convert the list of images to a numpy array\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "# Make predictions using the model\n",
    "predictions = model.predict(test_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# initialize a list to store the image names, prediction values, and labels\n",
    "results = []\n",
    "\n",
    "# convert predicted values to 'real' or 'fake' classification and store the results\n",
    "for i in range(len(predictions)):\n",
    "    image_name = [filename for filename in os.listdir(test_data_dir) if not filename.startswith(\".\")][i]\n",
    "    prediction_value = predictions[i][0]\n",
    "    label = 'fake' if prediction_value > 0.5 else 'real'\n",
    "    results.append([image_name, prediction_value, label])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output_file = \"predictions.csv\"\n",
    "with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"Image Name\", \"Prediction Value\", \"Classification\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"Predictions saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
